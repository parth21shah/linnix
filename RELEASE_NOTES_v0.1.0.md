# Linnix v0.1.0 - Initial Open Source Release ðŸŽ‰

**Release Date**: November 3, 2025

We're excited to announce the first open-source release of Linnix - eBPF-powered Linux observability with AI incident detection!

## ðŸš€ What's Included

### Core Components
- **cognitod** - eBPF monitoring daemon (101 MB Docker image)
- **linnix-cli** - Command-line interface for event streaming
- **linnix-reasoner** - AI-powered incident analysis
- **Docker Compose** - One-command setup for full stack

### AI Model (THIS RELEASE)
- **linnix-3b-distilled-q5_k_m.gguf** (2.1 GB)
  - Distilled from fine-tuned 7B teacher model
  - Quantized to Q5_K_M for optimal size/quality balance
  - Trained on system observability incidents
  - Runs on CPU (no GPU required)
  - Apache 2.0 licensed
  - **Download**: [Hugging Face Hub](https://huggingface.co/parth21shah/linnix-3b-distilled)

## ðŸ“¥ Quick Start

### Docker (Recommended - < 5 Minutes)

```bash
# Clone repository
git clone https://github.com/linnix-os/linnix.git
cd linnix

# Download and start (auto-downloads model)
./setup-llm.sh

# Verify services
curl http://localhost:3000/healthz  # cognitod
curl http://localhost:8090/health   # LLM server

# Get AI insights
curl http://localhost:3000/insights | jq
```

### Manual Installation

```bash
# 1. Download model from Hugging Face
wget https://huggingface.co/parth21shah/linnix-3b-distilled/resolve/main/linnix-3b-distilled-q5_k_m.gguf -P models/

# 2. Start with Docker Compose
docker-compose -f docker-compose.yml -f docker-compose.llm.yml up -d

# 3. Stream live events
docker exec -it linnix-cognitod linnix-cli stream
```

## âœ¨ Key Features

### eBPF Monitoring
- **Zero overhead**: <1% CPU usage
- **Process lifecycle**: Every fork, exec, exit captured
- **Telemetry**: CPU%, memory RSS per-process
- **Kernel support**: Linux 5.10+ with BTF

### AI-Powered Insights
- **Incident detection**: CPU spins, fork storms, OOM risks
- **Natural language**: "Java process holding 95% CPU for 3m"
- **Actionable**: Suggests remediation steps
- **Confidence scores**: Know when to trust predictions

### Production Ready
- **Battle-tested**: Running on multi-node clusters
- **Metrics**: Prometheus-compatible `/metrics` endpoint
- **Alerts**: Integration with PagerDuty, Slack
- **Scalable**: Sub-millisecond event processing

## ðŸ“Š Performance

| Metric | Value |
|--------|-------|
| Docker image size | 205 MB (cognitod + LLM) |
| Build time | ~5 minutes |
| Memory usage | ~50 MB (cognitod), ~2 GB (LLM) |
| Model inference | ~30 tokens/sec (8-core CPU) |
| Event throughput | 10,000+ events/sec |

## ðŸ†š vs. Alternatives

| Feature | Linnix | Datadog | Dynatrace |
|---------|--------|---------|-----------|
| **License** | Apache 2.0 (Open Source) | Proprietary | Proprietary |
| **AI Model** | Open, self-hostable | Closed, cloud-only | Closed, cloud-only |
| **Cost** | Free (self-host) | $15-31/host/month | $69-200/host/month |
| **Data privacy** | Stays on your infra | Sent to cloud | Sent to cloud |
| **Overhead** | <1% CPU (eBPF) | 3-5% CPU (agent) | 3-5% CPU (agent) |

## ðŸŽ¯ Use Cases

### 1. CPU Spin Detection
```bash
# Linnix detects:
# "java pid 4412 sustained 97% CPU for 3m"
# Suggests: "Capture async-profiler flame graph"
```

### 2. Fork Storm Prevention
```bash
# Detects 1000+ forks/sec from shell scripts
# Suggests: "Check cron jobs for infinite loops"
```

### 3. Memory Leak Isolation
```bash
# Tracks RSS growth per-process
# Identifies: "node pid 8821 grew 2GB in 10m"
```

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kernel Space (eBPF)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ tracepointâ”‚ â”‚ kprobes  â”‚            â”‚
â”‚  â”‚ fork/exec â”‚ â”‚ CPU/mem  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚           â”‚                   â”‚
â”‚    Perf Buffers (ring)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Space (cognitod)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Event Handler  â”‚  â”‚  Rule Engine   â”‚ â”‚
â”‚  â”‚ (Rust)         â”‚  â”‚  (YAML)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                 â”‚            â”‚
â”‚      HTTP API          Insights          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Layer (llama-server)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  linnix-3b-distilled-q5_k_m.gguf   â”‚  â”‚
â”‚  â”‚  (3B params, incident-tuned)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“¦ What's in the Box

### Binary Artifacts
- `cognitod` - Main daemon (Linux x86_64, arm64)
- `linnix-cli` - CLI tool
- `linnix-reasoner` - AI analysis tool

### Docker Images (ghcr.io)
- `linnixos/cognitod:latest` (101 MB)
- `linnixos/llama-cpp:latest` (104 MB)

### Model Files
- `linnix-3b-distilled-q5_k_m.gguf` (2.1 GB) - **NEW!**

### Documentation
- Quick start guide (README.md)
- Model training guide (docs/model-training.md)
- Dataset format (datasets/README.md)
- Prometheus integration (docs/prometheus-integration.md)

## ðŸ”’ Security & Privacy

### Data Stays Local
- All processing happens on your infrastructure
- No telemetry sent to external servers
- Model runs offline (no API calls)

### eBPF Safety
- Verified by kernel at load time
- Cannot crash kernel
- Read-only access to kernel data structures

### Minimal Privileges
- Runs as unprivileged user in Docker
- Only requires CAP_BPF capability
- No root password needed

## ðŸŒŸ Why Open Source?

We believe observability tools should be:
1. **Transparent**: You can audit every line of code
2. **Private**: Your data stays on your infrastructure  
3. **Extensible**: Fine-tune models on your incidents
4. **Affordable**: Free to self-host, pay only for support

## ðŸ›£ï¸ Roadmap

### v0.2.0 (Q1 2026)
- [ ] Multi-arch Docker images (arm64)
- [ ] Kubernetes operator
- [ ] GPU acceleration for LLM
- [ ] 7B model with better accuracy

### v0.3.0 (Q2 2026)
- [ ] Distributed tracing integration
- [ ] Cost attribution per-workload
- [ ] Anomaly detection (unsupervised)

### Enterprise (On Demand)
- Custom model training on your data
- SLA support (24/7)
- Advanced integrations (ServiceNow, Jira)

## ðŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for:
- Code style guidelines
- Development setup
- Signing the CLA
- Submitting pull requests

## ðŸ“œ License

- **Core Code**: AGPL-3.0-or-later (cognitod, CLI, reasoner)
- **eBPF Code**: GPL-2.0-or-later or MIT (kernel compatibility)
- **AI Model**: Apache-2.0 (linnix-3b-distilled)
- **Documentation**: CC-BY-4.0

See [LICENSE](LICENSE) for full details.

## ðŸ™ Credits

### Open Source Dependencies
- [Aya](https://github.com/aya-rs/aya) - Rust eBPF library
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - LLM inference engine
- [Tokio](https://tokio.rs) - Async Rust runtime

### Model Training
- Base student model: Qwen2.5-3B-Instruct (Apache 2.0)
- Teacher model: Fine-tuned Qwen2.5-7B on incident detection
- Training method: Knowledge distillation
- Training framework: Hugging Face Transformers + Axolotl
- Dataset: Proprietary incident dataset (synthetic + production)

### Community
- Beta testers who provided feedback
- Contributors who submitted PRs
- Users who reported bugs

## ðŸ“ž Support

- **Documentation**: [https://docs.linnix.io](https://docs.linnix.io)
- **Discord**: [https://discord.gg/linnix](https://discord.gg/linnix)
- **GitHub Issues**: [https://github.com/linnix-os/linnix/issues](https://github.com/linnix-os/linnix/issues)
- **Email**: support@linnix.io

## ðŸ† Powered by Linnix

Using Linnix in production? Add our badge to your README:

```markdown
[![Powered by Linnix](https://img.shields.io/badge/Powered%20by-Linnix-blue?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0iIzAwN0FGRiIvPgo8L3N2Zz4K)](https://github.com/linnix-os/linnix)
```

Renders as: [![Powered by Linnix](https://img.shields.io/badge/Powered%20by-Linnix-blue?style=flat-square)](https://github.com/linnix-os/linnix)

---

**Built with â¤ï¸ by the Linnix team**

[Website](https://linnix.io) â€¢ [Documentation](https://docs.linnix.io) â€¢ [Enterprise](https://linnix.io/enterprise)
